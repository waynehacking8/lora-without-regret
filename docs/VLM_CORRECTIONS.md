# VLM è…³æœ¬ä¿®æ­£èªªæ˜

## âš ï¸ é‡è¦ç™¼ç¾

ç¶“éæŸ¥é–± TRL å®˜æ–¹æ–‡æª”å’Œç¯„ä¾‹å¾Œï¼Œç™¼ç¾åŸå§‹ `sft_vlm_compare.py` æœ‰å¹¾å€‹é—œéµå•é¡Œéœ€è¦ä¿®æ­£ã€‚

---

## ğŸ”§ å¿…é ˆä¿®æ­£çš„å•é¡Œ

### 1. âŒ æ¨¡å‹è¼‰å…¥æ–¹å¼éŒ¯èª¤

**éŒ¯èª¤ä»£ç¢¼**:
```python
from transformers import LlavaForConditionalGeneration
model = LlavaForConditionalGeneration.from_pretrained(...)
```

**æ­£ç¢ºä»£ç¢¼**:
```python
from transformers import AutoModelForImageTextToText
model = AutoModelForImageTextToText.from_pretrained(...)
```

**åŸå› **: TRL SFTTrainer å®˜æ–¹å»ºè­°ä½¿ç”¨ `AutoModelForImageTextToText` ä»¥ç²å¾—æ›´å¥½çš„å…¼å®¹æ€§ã€‚

---

### 2. âŒ max_seq_length è¨­ç½®éŒ¯èª¤

**éŒ¯èª¤é…ç½®**:
```python
SFTConfig(
    max_seq_length=2048,  # âŒ æœƒæˆªæ–· image tokens!
    ...
)
```

**æ­£ç¢ºé…ç½®**:
```python
SFTConfig(
    max_length=None,  # âœ… å…è¨±å®Œæ•´åºåˆ—ï¼Œä¸æˆªæ–· image tokens
    ...
)
```

**åŸå› **: **VLM çš„ image tokens å¿…é ˆå®Œæ•´ä¿ç•™ï¼** æˆªæ–·æœƒå°è‡´è¨“ç·´éŒ¯èª¤æˆ–æ€§èƒ½ä¸‹é™ã€‚é€™æ˜¯ TRL å®˜æ–¹æ–‡æª”æ˜ç¢ºæŒ‡å‡ºçš„é—œéµé…ç½®ã€‚

**å®˜æ–¹èªªæ˜**:
> "For VLMs, truncating may remove image tokens, leading to errors during training. Set `max_length=None`."

---

### 3. âŒ gradient_checkpointing é…ç½®ç¼ºå¤±

**éŒ¯èª¤**: æ²’æœ‰è¨­ç½® `gradient_checkpointing_kwargs`

**æ­£ç¢ºé…ç½®**:
```python
training_args.gradient_checkpointing_kwargs = dict(use_reentrant=False)
```

**åŸå› **: VLM è¨“ç·´éœ€è¦ `use_reentrant=False` ä»¥é¿å…æ¢¯åº¦æª¢æŸ¥é»çš„å…¼å®¹æ€§å•é¡Œã€‚

---

### 3.5. âŒ **processing_class åƒæ•¸éŒ¯èª¤** â­ **æœ€é—œéµçš„ä¿®æ­£ï¼**

**éŒ¯èª¤ä»£ç¢¼**:
```python
trainer = SFTTrainer(
    model=model,
    processing_class=processor.tokenizer,  # âŒ éŒ¯èª¤ï¼
    ...
)
```

**æ­£ç¢ºä»£ç¢¼**:
```python
trainer = SFTTrainer(
    model=model,
    processing_class=processor,  # âœ… æ­£ç¢ºï¼å‚³éå®Œæ•´çš„ Processor
    ...
)
```

**éŒ¯èª¤åŸå› **:
TRL çš„ `SFTTrainer` ä½¿ç”¨ä»¥ä¸‹é‚è¼¯ä¾†åˆ¤æ–·æ¨¡å‹æ˜¯å¦ç‚º VLM:

```python
# TRL æºä»£ç¢¼é©—è­‰é‚è¼¯
if isinstance(processing_class, ProcessorMixin):
    self._is_vlm = True  # âœ… è­˜åˆ¥ç‚º VLM
elif isinstance(processing_class, PreTrainedTokenizerBase):
    self._is_vlm = False  # âŒ è­˜åˆ¥ç‚ºç´”æ–‡æœ¬æ¨¡å‹
```

ç•¶å‚³é `processor.tokenizer` æ™‚:
- `processor.tokenizer` æ˜¯ `PreTrainedTokenizerBase` å¯¦ä¾‹
- TRL åˆ¤å®š `_is_vlm = False`
- è³‡æ–™é›†æœ‰ `images` åˆ—ä½†æ¨¡å‹ä¸æ˜¯ VLM
- **æ‹‹å‡ºéŒ¯èª¤**: `ValueError: The dataset appears to be vision-related...but the provided model does not seem to be a vision-language model`

ç•¶å‚³é `processor` æ™‚:
- `processor` æ˜¯ `ProcessorMixin` å¯¦ä¾‹
- TRL åˆ¤å®š `_is_vlm = True`
- âœ… è¨“ç·´å¯ä»¥æ­£å¸¸é€²è¡Œ

**å®˜æ–¹ä¾†æº**: `trl/trainer/sft_trainer.py` ç¬¬ 743 è¡Œé©—è­‰é‚è¼¯

---

### 4. âš ï¸ è‡ªå®šç¾© data collator å¯èƒ½ä¸å¿…è¦

**ç•¶å‰åšæ³•**:
```python
def collate_fn(examples, processor):
    # è‡ªå®šç¾©è™•ç† images å’Œ text
    ...
```

**å®˜æ–¹åšæ³•**:
```python
# SFTTrainer è‡ªå‹•è™•ç†ï¼Œä¸éœ€è¦è‡ªå®šç¾© collator
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,  # ç›´æ¥å‚³éåŸå§‹è³‡æ–™é›†
    ...
)
```

**èªªæ˜**: TRL æœ‰å…§å»ºçš„ `DataCollatorForVisionLanguageModeling`ï¼Œæœƒè‡ªå‹•è™•ç† images åˆ—ã€‚é™¤éæœ‰ç‰¹æ®Šéœ€æ±‚ï¼Œå¦å‰‡ä¸éœ€è¦è‡ªå®šç¾©ã€‚

---

### 5. âš ï¸ è³‡æ–™é›†æ ¼å¼éœ€æ±‚

**SFTTrainer å° VLM çš„è¦æ±‚**:
1. è³‡æ–™é›†å¿…é ˆæœ‰ `images` åˆ—ï¼ˆPIL Image æ ¼å¼ï¼‰
2. å°è©±æ ¼å¼æ‡‰è©²æ˜¯æ¨™æº–çš„ messages æ ¼å¼
3. ä¸éœ€è¦æ‰‹å‹•é è™•ç†æˆ text æ ¼å¼

**ScienceQA æ ¼å¼æª¢æŸ¥**:
- âœ… æœ‰ `image` åˆ—
- â“ éœ€è¦è½‰æ›æˆ messages æ ¼å¼ï¼ˆå¾…ç¢ºèªï¼‰
- â“ åˆ—åæ˜¯ `image` é‚„æ˜¯ `images`ï¼ˆå¾…ç¢ºèªï¼‰

---

### 6. âš ï¸ bf16 vs fp16

**æŸ¥æ‰¾çµæœ**:
- éƒ¨åˆ†è³‡æºå»ºè­° LLaVA ä½¿ç”¨ **fp16** è€Œä¸æ˜¯ bf16
- LLaVA é è¨­å­˜å„²ç‚º float16

**å»ºè­°**:
- æ¸¬è©¦å…©ç¨®ç²¾åº¦
- æŸ¥çœ‹ LLaVA æ¨¡å‹å¡æ¨è–¦é…ç½®

---

### 7. âš ï¸ LoRA rank é…ç½®

**åŸé…ç½®**: rank=256 (éµå¾ª LoRA without Regret)

**å¸¸è¦‹ VLM é…ç½®**: rank=16-64

**å»ºè­°**:
- å…ˆç”¨ rank=128 æ¸¬è©¦
- å¦‚æœè¨˜æ†¶é«”å…è¨±ï¼Œå¯ä»¥ç”¨ 256
- rank=16 å¯èƒ½å¤ªå°ï¼Œç„¡æ³•å……åˆ†åˆ©ç”¨å®¹é‡

---

## ğŸ“‹ ä¿®æ­£å¾Œçš„é—œéµé…ç½®

```python
from transformers import AutoModelForImageTextToText
from trl import SFTTrainer, SFTConfig

# 1. ä½¿ç”¨æ­£ç¢ºçš„æ¨¡å‹é¡
model = AutoModelForImageTextToText.from_pretrained(
    "llava-hf/llava-1.5-7b-hf",
    torch_dtype=torch.float16,  # æˆ– bfloat16
    device_map="auto",
)

# 2. æ­£ç¢ºçš„è¨“ç·´é…ç½®
training_args = SFTConfig(
    output_dir="./output",
    max_length=None,  # â­ é—œéµï¼
    gradient_checkpointing=True,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=5,
)

# 3. è¨­ç½® gradient checkpointing
training_args.gradient_checkpointing_kwargs = dict(use_reentrant=False)

# 4. ç°¡åŒ–çš„è¨“ç·´å™¨
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,  # ç›´æ¥å‚³é
    peft_config=peft_config,
)
```

---

## ğŸ” éœ€è¦é©—è­‰çš„äº‹é …

### 1. ScienceQA è³‡æ–™é›†æ ¼å¼
```python
# æª¢æŸ¥è³‡æ–™é›†çµæ§‹
from datasets import load_dataset
ds = load_dataset("derek-thomas/ScienceQA", split="train[:10]")

# å¿…é ˆç¢ºèª:
# - æ˜¯å¦æœ‰ 'images' æˆ– 'image' åˆ—?
# - åœ–ç‰‡æ˜¯ PIL Image æ ¼å¼é‚„æ˜¯è·¯å¾‘?
# - éœ€è¦è½‰æ›æˆ messages æ ¼å¼å—?
```

### 2. å°è©±æ ¼å¼è½‰æ›
ScienceQA åŸå§‹æ ¼å¼:
```python
{
    "question": "...",
    "choices": ["A", "B", "C", "D"],
    "answer": 2,
    "image": PIL.Image
}
```

éœ€è¦è½‰æ›æˆ TRL æœŸæœ›çš„æ ¼å¼å—ï¼Ÿ
```python
{
    "messages": [
        {"role": "user", "content": "question + choices"},
        {"role": "assistant", "content": "answer"}
    ],
    "images": [PIL.Image]
}
```

### 3. Processor çš„ä½¿ç”¨
- SFTTrainer æœƒè‡ªå‹•å‘¼å« processor å—ï¼Ÿ
- é‚„æ˜¯éœ€è¦æ‰‹å‹•é è™•ç†ï¼Ÿ

---

## ğŸ¯ å»ºè­°çš„è¡Œå‹•æ­¥é©Ÿ

1. **å…ˆåœæ­¢ç•¶å‰ä¸‹è¼‰**ï¼ˆå¦‚æœéœ€è¦ï¼‰
2. **æª¢æŸ¥ ScienceQA è³‡æ–™é›†æ ¼å¼**
3. **åŸºæ–¼å®˜æ–¹ç¯„ä¾‹é‡å¯«è…³æœ¬**
4. **å‰µå»ºå°è¦æ¨¡æ¸¬è©¦**ï¼ˆ10-100 samplesï¼‰
5. **é©—è­‰è¨“ç·´å¯ä»¥æ­£å¸¸é‹è¡Œ**
6. **å†é€²è¡Œå®Œæ•´å¯¦é©—**

---

## ğŸ“š åƒè€ƒè³‡æº

- **TRL å®˜æ–¹ç¯„ä¾‹**: `trl/examples/scripts/sft_vlm.py`
- **TRL æ–‡æª”**: https://huggingface.co/docs/trl/sft_trainer
- **å®˜æ–¹æ•™ç¨‹**: https://www.philschmid.de/fine-tune-multimodal-llms-with-trl
- **LLaVA æ¨¡å‹å¡**: https://huggingface.co/llava-hf/llava-1.5-7b-hf

---

---

### 4. âŒ **PIL Image åºåˆ—åŒ–å•é¡Œ**

**éŒ¯èª¤**: dataset.map() æœƒè‡ªå‹•åºåˆ—åŒ– PIL Image æˆå­—å…¸

```python
formatted_dataset = dataset.map(
    format_fn,
    remove_columns=dataset.column_names,
    # éŒ¯èª¤:æ²’æœ‰æŒ‡å®š features,å°è‡´ PIL Image è¢«åºåˆ—åŒ–
)
# çµæœ:images = [{'bytes': b'...', 'path': None}]  # å­—å…¸,ä¸æ˜¯ PIL Image!
```

**æ­£ç¢ºåšæ³•**:

```python
from datasets import Features, Sequence, Value, Image as ImageFeature

features = Features({
    'messages': [{'role': Value('string'), 'content': Value('string')}],
    'images': Sequence(ImageFeature(decode=True))  # ä¿ç•™ PIL Images!
})

formatted_dataset = dataset.map(
    format_fn,
    remove_columns=dataset.column_names,
    features=features,  # âœ… æŒ‡å®š features ä¿ç•™ PIL Image
)
# çµæœ:images = [<PIL.Image>]  # çœŸæ­£çš„ PIL Image!
```

**éŒ¯èª¤è¨Šæ¯**:
```
TypeError: only a single or a list of entries is supported but got type=<class 'dict'>
```

---

## â³ ä¸‹ä¸€æ­¥

ç­‰å¾…ï¼š
1. âœ… æ¨¡å‹ä¸‹è¼‰å®Œæˆ
2. âœ… ç¢ºèª ScienceQA è³‡æ–™æ ¼å¼
3. âœ… ä¿®å¾©æ‰€æœ‰é—œéµå•é¡Œ
4. ğŸ”„ é©—è­‰è¨“ç·´å¯ä»¥æ­£å¸¸é‹è¡Œ

**ç‹€æ…‹**: ğŸ”„ æ­£åœ¨æ¸¬è©¦æœ€çµ‚ä¿®å¾©ç‰ˆæœ¬
**æœ€å¾Œæ›´æ–°**: 2025-10-29

**å·²ä¿®å¾©çš„é—œéµå•é¡Œ**:
1. âœ… processing_class åƒæ•¸(å‚³é processor è€Œé processor.tokenizer)
2. âœ… wandb é…ç½®(æ·»åŠ  report_to="none")
3. âœ… PIL Image åºåˆ—åŒ–(ä½¿ç”¨ features åƒæ•¸ä¿ç•™æ ¼å¼)
