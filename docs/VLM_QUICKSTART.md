# VLM å¯¦é©—å¿«é€Ÿå…¥é–€æŒ‡å—

## ğŸ¯ ç›®æ¨™

å°‡ "LoRA without Regret" æ–¹æ³•è«–æ‡‰ç”¨æ–¼ Vision-Language Models (VLMs)ï¼Œé©—è­‰å…¶åœ¨å¤šæ¨¡æ…‹ä»»å‹™ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

## ğŸ“‹ å‰ç½®éœ€æ±‚

### ç¡¬é«”éœ€æ±‚
- **GPU**: NVIDIA GPU with 16GB+ VRAM
  - æ¨è–¦: RTX 4090 (24GB), A5000 (24GB), A100 (40GB)
  - æœ€ä½: RTX 3090 (24GB) with 4-bit quantization
- **RAM**: 32GB+
- **å„²å­˜ç©ºé–“**: ~50GB (æ¨¡å‹ + è³‡æ–™é›† + çµæœ)

### è»Ÿé«”éœ€æ±‚
```bash
# Python 3.8+
python3 --version

# CUDA 11.8+ / 12.1+
nvidia-smi
```

## ğŸš€ å®‰è£ä¾è³´

```bash
# å‰µå»ºè™›æ“¬ç’°å¢ƒ (æ¨è–¦)
python3 -m venv venv
source venv/bin/activate

# å®‰è£æ ¸å¿ƒä¾è³´
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121

# å®‰è£ Transformers å’Œ TRL
pip install transformers>=4.40.0
pip install trl>=0.9.0
pip install peft>=0.10.0
pip install datasets>=2.18.0

# å®‰è£å…¶ä»–ä¾è³´
pip install accelerate>=0.27.0
pip install bitsandbytes>=0.43.0  # For 4-bit quantization
pip install pillow  # For image processing
pip install tqdm
pip install numpy
pip install matplotlib  # For visualization (optional)
```

## ğŸ“Š è³‡æ–™é›†æº–å‚™

ScienceQA æœƒè‡ªå‹•å¾ HuggingFace Hub ä¸‹è¼‰ï¼Œç„¡éœ€æ‰‹å‹•æº–å‚™ã€‚

```python
# æ¸¬è©¦è³‡æ–™é›†è¼‰å…¥
from datasets import load_dataset

ds = load_dataset("derek-thomas/ScienceQA", split="train[:10]")
print(f"Loaded {len(ds)} samples")
print(f"Columns: {ds.column_names}")
```

## ğŸƒ é‹è¡Œå¯¦é©—

### æ–¹æ³• 1: å®Œæ•´è‡ªå‹•åŒ–å¯¦é©— (æ¨è–¦)

```bash
# é‹è¡Œæ‰€æœ‰ 3 ç¨®æ–¹æ³• (LoRA-Attention, LoRA-All, Full FT)
bash run_scienceqa_experiments.sh
```

**é è¨ˆæ™‚é–“**: 3-6 å°æ™‚ (å–æ±ºæ–¼ GPU)

### æ–¹æ³• 2: å–®ç¨é‹è¡Œå¯¦é©—

#### Step 1: LoRA-Attention (åŸºæº–)

```bash
python3 sft_vlm_compare.py \
  --model llava-hf/llava-1.5-7b-hf \
  --dataset derek-thomas/ScienceQA \
  --method lora-attention \
  --out ./results/llava-scienceqa-lora-attention \
  --bf16 --use_4bit \
  --num_train_epochs 5 \
  --per_device_train_batch_size 1 \
  --grad_accum 4
```

**é è¨ˆæ™‚é–“**: 1-2 å°æ™‚

#### Step 2: LoRA-All (LoRA without Regret) â­

```bash
python3 sft_vlm_compare.py \
  --model llava-hf/llava-1.5-7b-hf \
  --dataset derek-thomas/ScienceQA \
  --method lora-all \
  --out ./results/llava-scienceqa-lora-all \
  --bf16 --use_4bit \
  --num_train_epochs 5 \
  --per_device_train_batch_size 1 \
  --grad_accum 4
```

**é è¨ˆæ™‚é–“**: 1-2 å°æ™‚

#### Step 3: Full Fine-Tuning

```bash
python3 sft_vlm_compare.py \
  --model llava-hf/llava-1.5-7b-hf \
  --dataset derek-thomas/ScienceQA \
  --method fullft \
  --out ./results/llava-scienceqa-fullft \
  --bf16 --use_4bit \
  --num_train_epochs 5 \
  --per_device_train_batch_size 1 \
  --grad_accum 4
```

**é è¨ˆæ™‚é–“**: 1-2 å°æ™‚

## ğŸ“ˆ è©•ä¼°æ¨¡å‹

è¨“ç·´å®Œæˆå¾Œï¼Œè©•ä¼°å„å€‹æ¨¡å‹ï¼š

```bash
# è©•ä¼° LoRA-Attention
python3 evaluate_vlm.py \
  --model_path ./results/llava-scienceqa-lora-attention \
  --base_model llava-hf/llava-1.5-7b-hf \
  --method lora-attention \
  --dataset derek-thomas/ScienceQA \
  --split "test[:500]" \
  --bf16 --use_4bit

# è©•ä¼° LoRA-All
python3 evaluate_vlm.py \
  --model_path ./results/llava-scienceqa-lora-all \
  --base_model llava-hf/llava-1.5-7b-hf \
  --method lora-all \
  --dataset derek-thomas/ScienceQA \
  --split "test[:500]" \
  --bf16 --use_4bit

# è©•ä¼° Full FT
python3 evaluate_vlm.py \
  --model_path ./results/llava-scienceqa-fullft \
  --base_model llava-hf/llava-1.5-7b-hf \
  --method fullft \
  --dataset derek-thomas/ScienceQA \
  --split "test[:500]" \
  --bf16 --use_4bit
```

## ğŸ“Š æŸ¥çœ‹çµæœ

### è¨“ç·´æŒ‡æ¨™
```bash
# æŸ¥çœ‹è¨“ç·´æŒ‡æ¨™ (loss, GPU memory, training time)
cat ./results/llava-scienceqa-lora-attention/metrics_lora-attention.json
cat ./results/llava-scienceqa-lora-all/metrics_lora-all.json
cat ./results/llava-scienceqa-fullft/metrics_fullft.json
```

### è©•ä¼°çµæœ
```bash
# æŸ¥çœ‹æ¸¬è©¦æº–ç¢ºç‡
cat ./results/llava-scienceqa-lora-attention/eval_results.json
cat ./results/llava-scienceqa-lora-all/eval_results.json
cat ./results/llava-scienceqa-fullft/eval_results.json
```

## ğŸ¨ æ¯”è¼ƒçµæœ

å»ºç«‹ç°¡å–®çš„æ¯”è¼ƒè…³æœ¬ï¼š

```python
import json

methods = ["lora-attention", "lora-all", "fullft"]

print(f"{'Method':<20} {'Accuracy':<12} {'GPU Memory':<12} {'Time (min)':<12}")
print("="*60)

for method in methods:
    # Load eval results
    eval_path = f"./results/llava-scienceqa-{method}/eval_results.json"
    metrics_path = f"./results/llava-scienceqa-{method}/metrics_{method}.json"

    with open(eval_path) as f:
        eval_data = json.load(f)
    with open(metrics_path) as f:
        metrics = json.load(f)

    accuracy = eval_data['accuracy']
    gpu_mem = max([m['gpu_memory_gb'] for m in metrics])
    time = metrics[-1]['elapsed_time_min']

    print(f"{method:<20} {accuracy:<12.2%} {gpu_mem:<12.1f} {time:<12.1f}")
```

## ğŸ”§ å¸¸è¦‹å•é¡Œ

### Q1: GPU è¨˜æ†¶é«”ä¸è¶³ (OOM)

**è§£æ±ºæ–¹æ¡ˆ**:
1. ä½¿ç”¨ 4-bit quantization: `--use_4bit`
2. æ¸›å°‘ batch size: `--per_device_train_batch_size 1`
3. å¢åŠ  gradient accumulation: `--grad_accum 8`
4. æ¸›å°‘ max_seq_length: `--max_seq_length 1024`

### Q2: è¨“ç·´é€Ÿåº¦å¤ªæ…¢

**è§£æ±ºæ–¹æ¡ˆ**:
1. ä½¿ç”¨æ›´å¼·çš„ GPU (A100 > RTX 4090 > RTX 3090)
2. å¢åŠ  batch size (å¦‚æœ GPU è¨˜æ†¶é«”å…è¨±)
3. æ¸›å°‘ training samples: `--max_samples 1000`

### Q3: å¦‚ä½•æ·»åŠ æ–°çš„ VLM è³‡æ–™é›†ï¼Ÿ

ç·¨è¼¯ `sft_vlm_compare.py` ä¸­çš„ `convert_to_vlm_format` å‡½æ•¸ï¼š

```python
def convert_to_vlm_format(example):
    """Add your dataset format here"""
    if "your_dataset_field" in example:
        # Your custom processing
        return {
            "image": example['image'],
            "text": your_formatted_text,
        }
    # ... existing formats
```

### Q4: LoRA rank æ‡‰è©²è¨­å¤šå°‘ï¼Ÿ

- **å°è³‡æ–™é›†** (<10K): rank=128-256
- **ä¸­å‹è³‡æ–™é›†** (10K-50K): rank=256-512
- **å¤§å‹è³‡æ–™é›†** (>50K): rank=512-1024

åŸå‰‡: rank è¶Šé«˜ï¼Œå®¹é‡è¶Šå¤§ï¼Œä½†è¨˜æ†¶é«”å’Œè¨“ç·´æ™‚é–“ä¹Ÿè¶Šå¤šã€‚

## ğŸ“š é€²éšé…ç½®

### èª¿æ•´å­¸ç¿’ç‡

```bash
# å¦‚æœè¨“ç·´ä¸ç©©å®šï¼Œé™ä½å­¸ç¿’ç‡
python3 sft_vlm_compare.py \
  --lora_lr 5e-5 \    # å¾ 1e-4 é™åˆ° 5e-5
  --fullft_lr 5e-6 \  # å¾ 1e-5 é™åˆ° 5e-6
  ...
```

### ä¸å‡çµ Vision Tower

```bash
# å¦‚æœæƒ³å¾®èª¿æ•´å€‹æ¨¡å‹ï¼ˆéœ€è¦æ›´å¤šè¨˜æ†¶é«”ï¼‰
python3 sft_vlm_compare.py \
  --freeze_vision_tower False \
  ...
```

### ä½¿ç”¨æ›´é«˜çš„ LoRA rank

```bash
python3 sft_vlm_compare.py \
  --lora_rank 512 \    # å¾ 256 æé«˜åˆ° 512
  --lora_alpha 32 \    # ç›¸æ‡‰èª¿æ•´ alpha
  ...
```

## ğŸ¯ é æœŸçµæœ

åŸºæ–¼åŸå§‹ LLM å¯¦é©—çš„ç™¼ç¾ï¼š

| æ–¹æ³• | é æœŸæº–ç¢ºç‡ | GPU è¨˜æ†¶é«” | è¨“ç·´æ™‚é–“ |
|------|-----------|-----------|---------|
| **LoRA-Attention** | ~60-70% | ~12-16 GB | 1-2 hrs |
| **LoRA-All** | ~70-75% | ~16-20 GB | 1-2 hrs |
| **Full FT** | ~70-75% | ~24-32 GB | 1.5-2.5 hrs |

**é—œéµç™¼ç¾** (é æœŸ):
- LoRA-All æ‡‰è©²èˆ‡ Full FT ç›¸ç•¶æˆ–æ›´å¥½
- LoRA-All ç¯€çœ ~30-40% GPU è¨˜æ†¶é«”
- åœ¨å°è³‡æ–™é›†ä¸Š LoRA-All å¯èƒ½å„ªæ–¼ Full FT

## ğŸ“ å¼•ç”¨

å¦‚æœé€™å€‹å¯¦é©—å°ä½ æœ‰å¹«åŠ©ï¼Œè«‹å¼•ç”¨åŸå§‹ç ”ç©¶ï¼š

```bibtex
@article{lora_without_regret,
  title={LoRA without Regret},
  author={HuggingFace Team},
  year={2024},
  url={https://huggingface.co/docs/trl/main/en/lora_without_regret}
}
```

## ğŸ’¬ æ”¯æ´

é‡åˆ°å•é¡Œï¼Ÿ
1. æª¢æŸ¥ [å¸¸è¦‹å•é¡Œ](#å¸¸è¦‹å•é¡Œ)
2. æŸ¥çœ‹åŸå§‹ README.md
3. æª¢æŸ¥ HuggingFace TRL æ–‡æª”

ç¥å¯¦é©—é †åˆ©ï¼ ğŸš€
