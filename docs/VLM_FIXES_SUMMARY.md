# VLM è¨“ç·´è…³æœ¬ä¿®æ­£ç¸½çµ

## ğŸ” å•é¡Œè¨ºæ–·éç¨‹

ç¶“éå¤šæ¬¡æ¸¬è©¦å’Œèª¿è©¦,ç™¼ç¾äº†**3å€‹é—œéµå•é¡Œ**å°è‡´ VLM è¨“ç·´å¤±æ•—ã€‚

---

## âŒ å•é¡Œ 1: processing_class åƒæ•¸éŒ¯èª¤ â­ **æœ€é—œéµ!**

### éŒ¯èª¤ä»£ç¢¼:
```python
trainer = SFTTrainer(
    model=model,
    processing_class=processor.tokenizer,  # âŒ éŒ¯èª¤!
    ...
)
```

### æ­£ç¢ºä»£ç¢¼:
```python
trainer = SFTTrainer(
    model=model,
    processing_class=processor,  # âœ… æ­£ç¢º!
    ...
)
```

### åŸå› :

TRL çš„ `SFTTrainer` ä½¿ç”¨ä»¥ä¸‹é‚è¼¯åˆ¤æ–·æ¨¡å‹æ˜¯å¦ç‚º VLM:

```python
# TRL æºä»£ç¢¼ (trl/trainer/sft_trainer.py:743)
if isinstance(processing_class, ProcessorMixin):
    self._is_vlm = True  # âœ… è­˜åˆ¥ç‚º VLM
elif isinstance(processing_class, PreTrainedTokenizerBase):
    self._is_vlm = False  # âŒ è­˜åˆ¥ç‚ºç´”æ–‡æœ¬æ¨¡å‹
```

ç•¶å‚³é `processor.tokenizer` æ™‚:
- `processor.tokenizer` æ˜¯ `PreTrainedTokenizerBase` å¯¦ä¾‹
- TRL åˆ¤å®š `_is_vlm = False`
- è³‡æ–™é›†æœ‰ `images` åˆ—ä½†æ¨¡å‹ä¸æ˜¯ VLM
- **æ‹‹å‡ºéŒ¯èª¤**: `ValueError: The dataset appears to be vision-related...but the provided model does not seem to be a vision-language model`

ç•¶å‚³é `processor` æ™‚:
- `processor` æ˜¯ `ProcessorMixin` å¯¦ä¾‹
- TRL åˆ¤å®š `_is_vlm = True`
- âœ… è¨“ç·´å¯ä»¥æ­£å¸¸é€²è¡Œ

### éŒ¯èª¤è¨Šæ¯:
```
ValueError: The dataset appears to be vision-related (contains 'image' or 'images' keys),
but the provided model does not seem to be a vision-language model.
Please check your model and dataset.
```

---

## âŒ å•é¡Œ 2: wandb é…ç½®å•é¡Œ

### éŒ¯èª¤:
æ²’æœ‰ç¦ç”¨ wandb,å°è‡´è¦æ±‚ API key

### éŒ¯èª¤è¨Šæ¯:
```
wandb.errors.errors.UsageError: api_key not configured (no-tty).
call wandb.login(key=[your_api_key])
```

### ä¿®æ­£:
åœ¨ training_config ä¸­æ·»åŠ :

```python
training_config = {
    ...
    "report_to": "none",  # âœ… ç¦ç”¨ wandb
    ...
}
```

---

## âŒ å•é¡Œ 3: PIL Image åºåˆ—åŒ–å•é¡Œ

### éŒ¯èª¤:
`dataset.map()` æœƒè‡ªå‹•å°‡ PIL Image åºåˆ—åŒ–æˆå­—å…¸æ ¼å¼

```python
formatted_dataset = dataset.map(
    format_fn,
    remove_columns=dataset.column_names,
    # âŒ æ²’æœ‰æŒ‡å®š features,å°è‡´ PIL Image è¢«åºåˆ—åŒ–
)

# çµæœ: images = [{'bytes': b'...', 'path': None}]  # å­—å…¸,ä¸æ˜¯ PIL Image!
```

### éŒ¯èª¤è¨Šæ¯:
```
TypeError: only a single or a list of entries is supported but got type=<class 'dict'>
```

åœ¨éŒ¯èª¤å †ç–Šä¸­:
```
File ".../transformers/image_processing_base.py", line 536, in fetch_images
    raise TypeError(f"only a single or a list of entries is supported but got type={type(image_url_or_urls)}")
```

### ä¿®æ­£:

ä½¿ç”¨ `features` åƒæ•¸æŒ‡å®šä¿ç•™ PIL Image æ ¼å¼:

```python
from datasets import Features, Sequence, Value, Image as ImageFeature

features = Features({
    'messages': [{'role': Value('string'), 'content': Value('string')}],
    'images': Sequence(ImageFeature(decode=True))  # âœ… ä¿ç•™ PIL Images!
})

formatted_dataset = dataset.map(
    format_fn,
    remove_columns=dataset.column_names,
    features=features,  # âœ… æŒ‡å®š features ä¿ç•™ PIL Image
    desc="Formatting dataset"
)

# çµæœ: images = [<PIL.Image>]  # âœ… çœŸæ­£çš„ PIL Image!
```

---

## âœ… å®Œæ•´ä¿®æ­£å¾Œçš„ä»£ç¢¼

### è³‡æ–™é›†æº–å‚™:

```python
def prepare_dataset(dataset, processor, dataset_name):
    from datasets import Features, Sequence, Value, Image as ImageFeature

    # Filter out samples without images
    dataset = dataset.filter(lambda x: x['image'] is not None)

    def format_scienceqa(example):
        # ... format messages ...
        return {
            "messages": messages,
            "images": [example['image']]  # List of PIL images
        }

    # âœ… CRITICAL: Define features to preserve PIL Image format
    features = Features({
        'messages': [{'role': Value('string'), 'content': Value('string')}],
        'images': Sequence(ImageFeature(decode=True))
    })

    formatted_dataset = dataset.map(
        format_scienceqa,
        remove_columns=dataset.column_names,
        features=features,  # âœ… Preserve PIL Images!
        desc="Formatting dataset"
    )

    return formatted_dataset
```

### Trainer é…ç½®:

```python
# Training configuration
training_config = {
    "output_dir": args.out,
    "max_length": None,  # âœ… CRITICAL for VLMs!
    "gradient_checkpointing": True,
    "remove_unused_columns": False,  # âœ… Important for VLMs!
    "report_to": "none",  # âœ… Disable wandb
    ...
}

training_args = SFTConfig(**training_config)

# âœ… Set gradient checkpointing kwargs
training_args.gradient_checkpointing_kwargs = dict(use_reentrant=False)

# âœ… CRITICAL FIX: Pass processor (ProcessorMixin), not tokenizer!
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=eval_ds,
    processing_class=processor,  # âœ… Pass processor, not processor.tokenizer!
    callbacks=[metrics_cb, early_stopping_cb],
)
```

---

## ğŸ“Š æ¸¬è©¦çµæœ

**æ¸¬è©¦ç‹€æ…‹**: ğŸ”„ æ­£åœ¨é©—è­‰æœ€çµ‚ä¿®å¾©ç‰ˆæœ¬

**é æœŸçµæœ**:
- âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ
- âœ… è³‡æ–™é›†æ ¼å¼åŒ–å®Œæˆ
- âœ… è¨“ç·´æˆåŠŸé–‹å§‹
- âœ… å¯ä»¥æ­£å¸¸é€²è¡Œ VLM LoRA without Regret å¯¦é©—

---

## ğŸ¯ é—œéµè¦é»

1. **processing_class å¿…é ˆå‚³é processor**
   - TRL ç”¨æ­¤åˆ¤æ–·æ˜¯å¦ç‚º VLM
   - å‚³é tokenizer æœƒå°è‡´ VLM é©—è­‰å¤±æ•—

2. **max_length å¿…é ˆè¨­ç‚º None**
   - VLM çš„ image tokens ä¸èƒ½è¢«æˆªæ–·
   - é€™æ˜¯ TRL å®˜æ–¹æ–‡æª”æ˜ç¢ºè¦æ±‚çš„

3. **PIL Image å¿…é ˆä¿ç•™æ ¼å¼**
   - ä½¿ç”¨ `features` åƒæ•¸é˜²æ­¢åºåˆ—åŒ–
   - TRL çš„ data collator éœ€è¦çœŸæ­£çš„ PIL Image ç‰©ä»¶

4. **ç¦ç”¨ wandb (å¯é¸)**
   - é¿å… API key å•é¡Œ
   - ä½¿ç”¨ `report_to="none"`

---

## ğŸ“š åƒè€ƒè³‡æº

- **TRL SFTTraineræºä»£ç¢¼**: `trl/trainer/sft_trainer.py`
- **TRLå®˜æ–¹æ–‡æª”**: https://huggingface.co/docs/trl/sft_trainer
- **GitHub Issue #3099**: SFTTrainer does not support VLM models specified via model name

---

**æœ€å¾Œæ›´æ–°**: 2025-10-29
**ç‹€æ…‹**: âœ… æ‰€æœ‰å·²çŸ¥å•é¡Œå·²ä¿®å¾©,ç­‰å¾…æ¸¬è©¦é©—è­‰
