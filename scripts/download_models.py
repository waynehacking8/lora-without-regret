#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é å…ˆä¸‹è¼‰ VLM æ¨¡å‹å’Œè³‡æ–™é›†
é¿å…è¨“ç·´æ™‚é‡è¤‡ä¸‹è¼‰ï¼Œç¯€çœæ™‚é–“
"""
import os
import argparse
from pathlib import Path
from huggingface_hub import login, snapshot_download
from datasets import load_dataset


def parse_args():
    p = argparse.ArgumentParser(description="é å…ˆä¸‹è¼‰ VLM æ¨¡å‹å’Œè³‡æ–™é›†")
    p.add_argument("--token_file", default="./huggingface_token.txt",
                   help="HuggingFace token æ–‡ä»¶è·¯å¾‘")
    p.add_argument("--cache_dir", default=None,
                   help="ä¸‹è¼‰ç›®éŒ„ (é è¨­ä½¿ç”¨ HuggingFace cache)")
    p.add_argument("--models", nargs="+",
                   default=["llava-hf/llava-1.5-7b-hf"],
                   help="è¦ä¸‹è¼‰çš„æ¨¡å‹åˆ—è¡¨")
    p.add_argument("--datasets", nargs="+",
                   default=["derek-thomas/ScienceQA"],
                   help="è¦ä¸‹è¼‰çš„è³‡æ–™é›†åˆ—è¡¨")
    p.add_argument("--skip_models", action="store_true",
                   help="è·³éæ¨¡å‹ä¸‹è¼‰")
    p.add_argument("--skip_datasets", action="store_true",
                   help="è·³éè³‡æ–™é›†ä¸‹è¼‰")
    return p.parse_args()


def load_token(token_file):
    """å¾æ–‡ä»¶è®€å– HuggingFace token"""
    token_path = Path(token_file)
    if not token_path.exists():
        print(f"âŒ Token æ–‡ä»¶ä¸å­˜åœ¨: {token_file}")
        print("è«‹å‰µå»ºæ–‡ä»¶ä¸¦å¡«å…¥ä½ çš„ HuggingFace token")
        return None

    with open(token_path, "r") as f:
        token = f.read().strip()

    if not token:
        print(f"âŒ Token æ–‡ä»¶ç‚ºç©º: {token_file}")
        return None

    return token


def download_model(model_name, cache_dir=None):
    """ä¸‹è¼‰ HuggingFace æ¨¡å‹"""
    print(f"\n{'='*60}")
    print(f"ä¸‹è¼‰æ¨¡å‹: {model_name}")
    print(f"{'='*60}")

    try:
        # Download entire model repository
        local_path = snapshot_download(
            repo_id=model_name,
            cache_dir=cache_dir,
            resume_download=True,
            local_files_only=False,
        )
        print(f"âœ… æ¨¡å‹ä¸‹è¼‰å®Œæˆ!")
        print(f"   è·¯å¾‘: {local_path}")
        return True
    except Exception as e:
        print(f"âŒ ä¸‹è¼‰å¤±æ•—: {e}")
        return False


def download_dataset(dataset_name, cache_dir=None):
    """ä¸‹è¼‰ HuggingFace è³‡æ–™é›†"""
    print(f"\n{'='*60}")
    print(f"ä¸‹è¼‰è³‡æ–™é›†: {dataset_name}")
    print(f"{'='*60}")

    try:
        # Download all splits
        print("æ­£åœ¨ä¸‹è¼‰ train split...")
        train_ds = load_dataset(
            dataset_name,
            split="train",
            cache_dir=cache_dir,
        )
        print(f"âœ“ Train: {len(train_ds)} samples")

        print("æ­£åœ¨ä¸‹è¼‰ validation split...")
        val_ds = load_dataset(
            dataset_name,
            split="validation",
            cache_dir=cache_dir,
        )
        print(f"âœ“ Validation: {len(val_ds)} samples")

        print("æ­£åœ¨ä¸‹è¼‰ test split...")
        test_ds = load_dataset(
            dataset_name,
            split="test",
            cache_dir=cache_dir,
        )
        print(f"âœ“ Test: {len(test_ds)} samples")

        print(f"âœ… è³‡æ–™é›†ä¸‹è¼‰å®Œæˆ!")
        return True
    except Exception as e:
        print(f"âŒ ä¸‹è¼‰å¤±æ•—: {e}")
        return False


def main():
    args = parse_args()

    print("="*60)
    print("VLM æ¨¡å‹å’Œè³‡æ–™é›†ä¸‹è¼‰å·¥å…·")
    print("="*60)

    # Load and login with token
    print(f"\nè®€å– HuggingFace token: {args.token_file}")
    token = load_token(args.token_file)

    if token:
        print("ç™»å…¥ HuggingFace Hub...")
        try:
            login(token=token)
            print("âœ… ç™»å…¥æˆåŠŸ!")
        except Exception as e:
            print(f"âŒ ç™»å…¥å¤±æ•—: {e}")
            print("ç¹¼çºŒå˜—è©¦ä¸‹è¼‰...")
    else:
        print("âš ï¸  æœªæä¾› tokenï¼ŒæŸäº›æ¨¡å‹å¯èƒ½ç„¡æ³•ä¸‹è¼‰")

    # Summary
    total_downloads = 0
    successful_downloads = 0

    # Download models
    if not args.skip_models:
        print(f"\n{'='*60}")
        print(f"é–‹å§‹ä¸‹è¼‰ {len(args.models)} å€‹æ¨¡å‹")
        print(f"{'='*60}")

        for model_name in args.models:
            total_downloads += 1
            if download_model(model_name, args.cache_dir):
                successful_downloads += 1

    # Download datasets
    if not args.skip_datasets:
        print(f"\n{'='*60}")
        print(f"é–‹å§‹ä¸‹è¼‰ {len(args.datasets)} å€‹è³‡æ–™é›†")
        print(f"{'='*60}")

        for dataset_name in args.datasets:
            total_downloads += 1
            if download_dataset(dataset_name, args.cache_dir):
                successful_downloads += 1

    # Final summary
    print(f"\n{'='*60}")
    print("ä¸‹è¼‰å®Œæˆ!")
    print(f"{'='*60}")
    print(f"æˆåŠŸ: {successful_downloads}/{total_downloads}")

    if successful_downloads == total_downloads:
        print("\nâœ… æ‰€æœ‰ä¸‹è¼‰æˆåŠŸ!")
        print("\nä¸‹ä¸€æ­¥:")
        print("  é‹è¡Œå¯¦é©—: bash run_scienceqa_experiments.sh")
    else:
        print(f"\nâš ï¸  éƒ¨åˆ†ä¸‹è¼‰å¤±æ•— ({total_downloads - successful_downloads} å€‹)")
        print("è«‹æª¢æŸ¥éŒ¯èª¤è¨Šæ¯ä¸¦é‡è©¦")

    # Show cache location
    cache_dir = args.cache_dir or os.environ.get("HF_HOME") or os.path.expanduser("~/.cache/huggingface")
    print(f"\nğŸ“ ä¸‹è¼‰ä½ç½®: {cache_dir}")


if __name__ == "__main__":
    main()


# Usage examples:

# åŸºæœ¬ç”¨æ³• (ä½¿ç”¨é è¨­ token æ–‡ä»¶)
# python3 download_models.py

# æŒ‡å®š token æ–‡ä»¶
# python3 download_models.py --token_file /path/to/token.txt

# åªä¸‹è¼‰æ¨¡å‹
# python3 download_models.py --skip_datasets

# åªä¸‹è¼‰è³‡æ–™é›†
# python3 download_models.py --skip_models

# ä¸‹è¼‰å¤šå€‹æ¨¡å‹
# python3 download_models.py --models llava-hf/llava-1.5-7b-hf llava-hf/llava-1.5-13b-hf

# æŒ‡å®šä¸‹è¼‰ç›®éŒ„
# python3 download_models.py --cache_dir /path/to/cache
